# ================================
# Imports needed for decomposition
# ================================
# import pandas as pd
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler
# from sklearn.decomposition import PCA, TruncatedSVD, NMF, LatentDirichletAllocation

# PCA (Principal Component Analysis)
# Encode â†’ Use OneHotEncoder or LabelEncoder if categories, else none
# Scale â†’ âœ… Use StandardScaler (PCA needs even scales)
# Split â†’ ğŸš« Not used (no target variable)
# Model â†’ Use PCA() from sklearn.decomposition
# Evaluate â†’ Explained Variance Ratio (how much info is kept)
# Notes: Turns many columns into fewer â€œsummary columns.â€
# Example: Instead of 100 test scores, PCA might find 2â€“3 â€œmain skills.â€
# Scores: >90% variance Excellent ğŸš€, 75â€“90% Good ğŸ‘, 50â€“75% Fair ğŸ˜, <50% Poor ğŸ‘
# Use when: You have too many numeric features and want to shrink them.

# Truncated SVD (Singular Value Decomposition)
# Encode â†’ Use OneHotEncoder if categories, else none (often used with text TF-IDF, no encoder needed)
# Scale â†’ âœ… Use StandardScaler or MinMaxScaler
# Split â†’ ğŸš« Not used
# Model â†’ Use TruncatedSVD() from sklearn.decomposition
# Evaluate â†’ Explained Variance Ratio
# Notes: Works like PCA but better for text data with lots of zeros.
# Example: From thousands of words, it can find 20â€“50 â€œmain concepts.â€
# Scores: Same ranges as PCA
# Use when: Data is very big and sparse (like bag-of-words or TF-IDF).

# NMF (Non-negative Matrix Factorization)
# Encode â†’ Use OneHotEncoder if categories, else none (needs numeric and positive values)
# Scale â†’ âœ… Use MinMaxScaler (keeps numbers positive)
# Split â†’ ğŸš« Not used
# Model â†’ Use NMF() from sklearn.decomposition
# Evaluate â†’ Reconstruction Error (smaller = better)
# Notes: Breaks data into smaller parts, no negatives allowed.
# Example: A face picture can be split into â€œnose,â€ â€œeyes,â€ â€œmouth.â€
# Scores: Error close to 0 = Excellent ğŸš€, higher error = worse
# Use when: Features canâ€™t be negative (word counts, money, pixels).

# Latent Dirichlet Allocation (LDA)
# Encode â†’ Usually none (works with raw counts or TF-IDF directly)
# Scale â†’ ğŸš« No scaling needed
# Split â†’ ğŸš« Not used
# Model â†’ Use LatentDirichletAllocation() from sklearn.decomposition
# Evaluate â†’ Log-Likelihood (higher = better), Perplexity (lower = better)
# Notes: Finds hidden â€œtopicsâ€ in text.
# Example: News articles might split into topics like â€œsports,â€ â€œpolitics,â€ â€œmovies.â€
# Scores: Lower perplexity = better topics; high perplexity = weak grouping
# Use when: You want to discover topics in text data.
