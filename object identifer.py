
#########   OBJECT DATA TYPES ################
# Object dtypes â†’ can mean categories or free text

# Nominal â†’ OneHotEncoder / get_dummies
# How to identify:
# - Text/labels with no natural order.
# - Nominal = labels/IDs with no ranking.
# - Examples: Department (Finance, HR, IT), Doc Type (PR, MIPR, IAA).
# - Ask: â€œCan I rank these?â€ â†’ if No, itâ€™s Nominal.
# What to use:
# - OneHotEncoder (sklearn) â†’ flexible, works in pipelines
# - pd.get_dummies() (pandas) â†’ quick and easy
# - Target encoding (advanced) â†’ replace each category with mean of target
# - Hashing trick (advanced) â†’ good when there are many categories

# Ordinal â†’ OrdinalEncoder (or manual mapping)
# How to identify:
# - Categories with a clear ranking or progression.
# - Examples: Job Level (Entry < Mid < Senior < Executive), Status (Incomplete < In Process < Approved).
# - Ask: â€œDo these categories have a natural sequence?â€ â†’ if Yes, itâ€™s Ordinal.
# What to use:
# - OrdinalEncoder (sklearn) â†’ assigns numbers that keep order
# - Manual mapping â†’ {'Entry':1, 'Mid':2, 'Senior':3, 'Executive':4}
# - Target encoding (alternative) â†’ if numbers need to reflect outcome

# Boolean â†’ simple 0/1 conversion
# How to identify:
# - Exactly two unique values (Yes/No, True/False, Approved/Not Approved).
# - Can be treated as a binary flag.
# - Ask: â€œDoes this column only have two options?â€ â†’ if Yes, itâ€™s Boolean.
# What to use:
# - astype(int) â†’ True=1, False=0
# - Replace mapping â†’ {'Yes':1, 'No':0} or {'Approved':1, 'Rejected':0}
# - LabelEncoder (sklearn) â†’ also works, but overkill

# ğŸ”¹ CountVectorizer
# - What it does: turns text into raw counts of words
# - When to use:
#   âœ” Small/simple datasets
#   âœ” When word frequency itself matters
#   âœ” Quick testing before using TF-IDF
# - When NOT to use:
#   âœ˜ Large text datasets (common words take over)
#   âœ˜ When you care about importance, not just counts
# - Options you can add:
#   stop_words='english'   â†’ remove common words like "the, is, and"
#   ngram_range=(1,2)      â†’ use single words and pairs of words
#   max_features=1000      â†’ keep only the top 1000 words
#   min_df=2               â†’ ignore words that appear in fewer than 2 documents
#   max_df=0.8             â†’ ignore words that appear in more than 80% of documents

# ğŸ”¹ TfidfVectorizer
# - What it does: turns text into weighted scores (importance of words)
# - When to use:
#   âœ” Bigger datasets with many words
#   âœ” Finance/legal/medical/business text where meaning matters
#   âœ” When you want rare but important words to stand out
# - When NOT to use:
#   âœ˜ Very tiny datasets (can over-penalize)
#   âœ˜ If raw counts are enough (simple tasks)
# - Options you can add:
#   stop_words='english'   â†’ remove common words like "the, is, and"
#   ngram_range=(1,2)      â†’ use single words and pairs of words
#   max_features=1000      â†’ keep only the top 1000 words
#   min_df=2               â†’ ignore words that appear in fewer than 2 documents
#   max_df=0.8             â†’ ignore words that appear in more than 80% of documents
#   use_idf=True           â†’ use IDF weighting (default: True)
#   smooth_idf=True        â†’ avoid zero errors (default: True)
#   norm='l2'              â†’ normalize results (default: 'l2')



########### NUMERIC DATA TYPES #############
# Original values
# Salary: [60,000, 80,000, 100,000]   # real salaries (big numbers)
# Bonus:  [2,000, 5,000, 10,000]     # real bonuses (smaller numbers)

# Problem
# Salary â†’ much bigger numbers (looks more important)
# Bonus â†’ smaller numbers (looks less important)
# Computer â†’ gets tricked into thinking Salary matters more

# StandardScaler (center around 0)
# Salary: [-1.0, 0.0, 1.0]   # small salary = -1, medium salary = 0, big salary = +1
# Bonus:  [-0.9, 0.0, 0.9]   # small bonus = -0.9, medium bonus = 0, big bonus = +0.9

# MinMaxScaler (squish into 0â€“1)
# Salary: [0.0, 0.5, 1.0]    # lowest salary = 0, middle salary = 0.5, highest salary = 1
# Bonus:  [0.0, 0.375, 1.0]  # lowest bonus = 0, middle bonus â‰ˆ 0.38, highest bonus = 1

# Easy way to remember
# Salary with StandardScaler â†’ see-saw around 0
# Bonus with StandardScaler â†’ also see-saw around 0
# Salary with MinMaxScaler â†’ ruler from 0 to 1
# Bonus with MinMaxScaler â†’ also ruler from 0 to 1

############# DATES DATA TYPES #############
# Datetime dtypes â†’ values stored as dates (e.g., 2020-01-01, 2025-09-13)
# Dates look like calendar values, not numbers the computer can use

# Problem:
# Regression cannot use raw dates directly   # the computer doesnâ€™t understand â€œJanuary 1st, 2020â€
# Computer only works with numbers           # so we must turn dates into numbers

# Fix â†’ turn dates into numeric features
# This means break the date into useful number parts

# Year â†’ pull the year out of the date
# Example: 2020-01-01 â†’ 2020
# When to use: if the actual year matters (e.g., budget year, fiscal year)
# When not to use: if year itself has no meaning (e.g., invoice date, where only age matters)

# Month â†’ pull the month number
# Example: 2020-01-01 â†’ 1
# When to use: if seasonality matters (e.g., sales higher in December)
# When not to use: if month doesnâ€™t affect the outcome

# Day â†’ pull the day number
# Example: 2020-01-01 â†’ 1
# When to use: if day of month is important (e.g., salary paid on 1st vs 15th)
# When not to use: if day has no effect

# Day of week â†’ 0=Monday, 6=Sunday
# Example: 2020-01-01 (Wednesday) â†’ 2
# When to use: if weekdays/weekends affect outcome (e.g., expenses higher on weekends)
# When not to use: if data is not tied to weekly patterns

# Weekend flag â†’ is it weekend? Yes=1, No=0
# Example: 2020-01-04 (Saturday) â†’ 1
# When to use: if weekend/weekday difference matters (e.g., travel, retail sales)
# When not to use: if outcome is unrelated to weekends

# Tenure / Age of record â†’ today â€“ date in years or days
# Example: (2025-09-13 â€“ 2020-01-01) â†’ 5 years
# When to use: if â€œhow oldâ€ something is matters (e.g., employee tenure, contract age)
# When not to use: if exact age has no effect on outcome

# Easy way to remember:
# Dates â†’ break into pieces (year, month, day, weekday)   # cut the date into smaller number parts
# Or measure "how long ago" something happened (tenure)   # count years/days from then to now