# ================================
# Imports needed for regression
# ================================
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.svm import SVR
# from sklearn.neural_network import MLPRegressor
# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Why regression is used:
# To predict a number or value (like price, age, income, temperature).

# When to use:
# When your target (the answer you want) is a number, not a category.
# Example: Predict how much money a movie will make, not if it is a hit or flop.

# Rule:
# Use regression when the answer is a number. Use classification when the answer is a category.



# Linear Regression
# Encode â†’ convert categories (LabelEncoder, OneHotEncoder)
# Scale â†’ âœ… StandardScaler (best for normal data), MinMaxScaler (if bounded range is preferred)
# Split â†’ train_test_split(features, target)
# Model â†’ LinearRegression()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Fits a straight line to predict a number
# Scores: RÂ² closer to 1 = stronger; MAE Ã· Mean:
#   <10% Excellent ðŸš€, 10â€“25% Good ðŸ‘, 25â€“50% Fair ðŸ˜, >50% Poor ðŸ‘Ž
# Use when: Data is numeric and roughly linear

# Polynomial Regression
# Encode â†’ same as above
# Scale â†’ âœ… StandardScaler (preferred), MinMaxScaler (alternative)
# Split â†’ train_test_split
# Model â†’ PolynomialFeatures + LinearRegression
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Extends linear regression by adding polynomial terms (xÂ², xÂ³, etc.)
# Scores: Same ranges as above
# Use when: Relationship between features and target is curved, not straight

# Ridge Regression
# Encode â†’ same as above
# Scale â†’ âœ… StandardScaler (best), MinMaxScaler (can also be used)
# Split â†’ train_test_split
# Model â†’ Ridge()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Linear regression with penalty on large coefficients (L2)
# Scores: Same ranges as above
# Use when: Features are correlated and you want to reduce overfitting

# Lasso Regression
# Encode â†’ same as above
# Scale â†’ âœ… StandardScaler (best), MinMaxScaler (alternative)
# Split â†’ train_test_split
# Model â†’ Lasso()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Linear regression with stronger penalty (L1) that can zero-out coefficients
# Scores: Same ranges as above
# Use when: You want both prediction and automatic feature selection

# ElasticNet Regression
# Encode â†’ same as above
# Scale â†’ âœ… StandardScaler (best), MinMaxScaler (alternative)
# Split â†’ train_test_split
# Model â†’ ElasticNet()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Mix of Ridge and Lasso penalties (L1 + L2)
# Scores: Same ranges as above
# Use when: Both correlation and feature selection matter

# Decision Tree Regressor
# Encode â†’ same as above
# Scale â†’ ðŸš« No scaling needed (trees split directly on values)
# Split â†’ train_test_split
# Model â†’ DecisionTreeRegressor()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Splits data into groups, predicts a number by averaging values
# Scores: Same ranges as above
# Use when: Data is non-linear and you want interpretability

# Random Forest Regressor
# Encode â†’ same as above
# Scale â†’ ðŸš« No scaling needed
# Split â†’ train_test_split
# Model â†’ RandomForestRegressor()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Builds many trees and averages them â†’ more stable, less overfitting
# Scores: RÂ² usually higher than a single tree; Error % same ranges
# Use when: You want stronger performance than one tree

# Gradient Boosting Regressor
# Encode â†’ same as above
# Scale â†’ ðŸš« No scaling needed
# Split â†’ train_test_split
# Model â†’ GradientBoostingRegressor()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Builds trees one by one, each fixing mistakes of the previous
# Scores: Often very high RÂ²; Error % same ranges
# Use when: You want high accuracy, even if training is slower

# K-Nearest Neighbors (KNN) Regressor
# Encode â†’ same as above
# Scale â†’ âœ… MinMaxScaler (best for distance), StandardScaler (works too)
# Split â†’ train_test_split
# Model â†’ KNeighborsRegressor()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Predicts a number by averaging nearby points
# Scores: Same ranges as above
# Use when: Relationship depends on local neighborhoods, not a formula

# Support Vector Regressor (SVR)
# Encode â†’ same as above
# Scale â†’ âœ… StandardScaler (preferred), MinMaxScaler (alternative)
# Split â†’ train_test_split
# Model â†’ SVR()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Finds a margin that best fits the data within a tolerance
# Scores: Same ranges as above
# Use when: Data has outliers or is high-dimensional

# Neural Network Regressor (MLPRegressor)
# Encode â†’ same as above
# Scale â†’ âœ… MinMaxScaler (preferred for neural nets), StandardScaler (works too)
# Split â†’ train_test_split
# Model â†’ MLPRegressor()
# Evaluate â†’ RÂ², MAE, MSE, MAE Ã· Mean (% error)
# Notes: Layers of nodes learn complex patterns
# Scores: Same ranges as above
# Use when: Data is large, complex, and non-linear

import pandas as pd
df = pd.read_csv("c:/Users/anton/OneDrive/school/wk2_movies.csv")

print(df.head().to_string())